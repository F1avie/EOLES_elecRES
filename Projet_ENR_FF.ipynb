{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMNupS2uBmLbGbJx3wp1m+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/F1avie/EOLES_elecRES/blob/master/Projet_ENR_FF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projet 5A ENR\n",
        "\n",
        "## Apprendre la stratégie journalière de stockage\n",
        "### Mise en place d'un réseau de neurone"
      ],
      "metadata": {
        "id": "-6N85Q6oviCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectifs : \n",
        "- Estimer le coût d’un mix énergétique de manière rapide en limitant le temps de calcul de l'optimisation du stockage qui se fait aujourd’hui heure par heure. "
      ],
      "metadata": {
        "id": "VB23RE10sOIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L’entrée du réseau peut être par exemple :\n",
        "1) L’état des stocks des différents moyens de stockage (en valeur absolue ou en pourcentage) à 0h\n",
        "2) La production nette de la journée (production totale de la journée - consommation totale)\n",
        "3) le jour de l’année (entre 1 et 365)\n",
        "Et les sorties seraient : l’état des stocks en fin journée des différents moyens de stockages et le cout de stockage de la journée.\n",
        "Ceci n’st qu’une proposition. On peut aussi mettre en entrée, la variance de la production journalière qui peut avoir un rôle."
      ],
      "metadata": {
        "id": "zCXPHlmytGVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import ticker\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "RBQ4OGW1spZS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prétraitement des données\n",
        "\n",
        "- il faut genérer les données avec les modèles \n",
        "\n",
        "- Quel mix choisir ? Le choisir et l'importer ici =) \n",
        "\n",
        "- Le cout de stockage n'est t'il pas aussi dépendant du profil de consommation ? (estimation ADEME, Negawhatt, RTE ?) réponse : le profil de consommation intervient dans la production de la journée nette (qui n'est pas un partie logique)\n",
        "\n",
        "- Comment on fait pour connaître la production de la journée idem pour la consommation ? Ducoup c'est online ? Quelle idée derrière la variance de la production journalière (+ de variance plus de stockage ?) ? \n",
        "- C'est pas un peu con de connaître exactement la production de la journée ? parceque ducoup le stock prédis ca va être sensiblement stock d'entree + prod de la journée : un peu étrange selon Flavie\n",
        "\n",
        "\n",
        "- produire les données d'apprentissage contenant\n",
        "\n",
        "    - 1 jour de l'année entre 1 et 365\n",
        "    - état des stocks à h0\n",
        "    - production de la journée\n",
        "    - variance production journalière.\n",
        "    - variance consomation journalière utile ? (peut être bien que oui aussi)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx_GNyULvaxz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IcT26aKPsgpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model de réseau de neurones\n",
        "\n",
        "Il s'agit d'un truc au hasard d'internet à affiner \n",
        "d'internet : \n",
        "\n",
        "'Pour la couche Dropout, on a diminué de 30% le nombre des données d’entrée afin d’éviter le phénomène du overfitting. La graine prend une valeur de 2 pour avoir des résultats plus reproductibles.' from https://www.cours-gratuit.com/tutoriel-python/tutoriel-python-matriser-les-rseaux-de-neurones-avec-keras"
      ],
      "metadata": {
        "id": "kHeeuKcZ0kzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "layers.Dense(64, activation = 'relu', input_shape = [train.shape[1]]),\n",
        "layers.Dropout(0.3, seed = 2),\n",
        "layers.Dense(64, activation = 'swish'),\n",
        "layers.Dense(64, activation = 'relu'),\n",
        "layers.Dense(64, activation = 'swish'),\n",
        "layers.Dense(64, activation = 'relu'),\n",
        "layers.Dense(64, activation = 'swish'),\n",
        "layers.Dense(1)])"
      ],
      "metadata": {
        "id": "o8h-GPDM0iY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Liste des optimizer à choisir\n",
        "\n",
        "class Adadelta: Optimizer that implements the Adadelta algorithm.\n",
        "\n",
        "class Adagrad: Optimizer that implements the Adagrad algorithm.\n",
        "\n",
        "class Adam: Optimizer that implements the Adam algorithm.\n",
        "\n",
        "class Adamax: Optimizer that implements the Adamax algorithm.\n",
        "\n",
        "class Ftrl: Optimizer that implements the FTRL algorithm.\n",
        "\n",
        "class Nadam: Optimizer that implements the NAdam algorithm.\n",
        "\n",
        "class Optimizer: Base class for Keras optimizers.\n",
        "\n",
        "class RMSprop: Optimizer that implements the RMSprop algorithm.\n",
        "\n",
        "class SGD: Gradient descent (with momentum) optimizer."
      ],
      "metadata": {
        "id": "8g0RJw0715PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Liste des losses à choisir\n",
        "\n",
        "class BinaryCrossentropy: Computes the cross-entropy loss between true labels and predicted labels.\n",
        "\n",
        "class BinaryFocalCrossentropy: Computes the focal cross-entropy loss between true labels and predictions.\n",
        "\n",
        "class CategoricalCrossentropy: Computes the crossentropy loss between the labels and predictions.\n",
        "\n",
        "class CategoricalHinge: Computes the categorical hinge loss between y_true and y_pred.\n",
        "\n",
        "class CosineSimilarity: Computes the cosine similarity between labels and predictions.\n",
        "\n",
        "class Hinge: Computes the hinge loss between y_true and y_pred.\n",
        "\n",
        "class Huber: Computes the Huber loss between y_true and y_pred.\n",
        "\n",
        "class KLDivergence: Computes Kullback-Leibler divergence loss between y_true and y_pred.\n",
        "\n",
        "class LogCosh: Computes the logarithm of the hyperbolic cosine of the prediction error.\n",
        "\n",
        "class Loss: Loss base class.\n",
        "\n",
        "class MeanAbsoluteError: Computes the mean of absolute difference between labels and predictions.\n",
        "\n",
        "class MeanAbsolutePercentageError: Computes the mean absolute percentage error between y_true and y_pred.\n",
        "\n",
        "class MeanSquaredError: Computes the mean of squares of errors between labels and predictions.\n",
        "\n",
        "class MeanSquaredLogarithmicError: Computes the mean squared logarithmic error between y_true and y_pred.\n",
        "\n",
        "class Poisson: Computes the Poisson loss between y_true and y_pred.\n",
        "\n",
        "class Reduction: Types of loss reduction.\n",
        "\n",
        "class SparseCategoricalCrossentropy: Computes the crossentropy loss between the labels and predictions.\n",
        "\n",
        "class SquaredHinge: Computes the squared hinge loss between y_true and y_pred.\n",
        "\n",
        "## Choisir une metrics"
      ],
      "metadata": {
        "id": "9gNTE52D3sha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimiseur = tf.keras.optimizers.RMSprop(learning_rate = 0.001)\n",
        "modele.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
        "optimizer = optimiseur,\n",
        "metrics = ['mae'])\n",
        "\n",
        "training = modele.fit (train, train_cible, epochs = 70, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "bEu1swwp1Qn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dX-goASl3rah"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}